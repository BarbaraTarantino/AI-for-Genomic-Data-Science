{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef8671d9",
   "metadata": {},
   "source": [
    "# Biomedical LLMs — Practical Example with BioGPT and PubMedBERT\n",
    "\n",
    "This notebook demonstrates in a **simple and educational** manner how to use two pre-trained biomedical language models with **Hugging Face**:\n",
    "\n",
    "- **BioGPT** → text generation (medical answers or explanations)  \n",
    "- **PubMedBERT** → understanding and classification of biomedical text\n",
    "\n",
    "We will consistently follow the logical flow:  \n",
    "**Text → Tokenizer → Model → Readable Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536fe19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries successfully imported.\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers torch datasets evaluate tqdm ipywidgets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "\n",
    "print(\"Libraries successfully imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6490102",
   "metadata": {},
   "source": [
    "## What is a Biomedical Model\n",
    "\n",
    "**Biomedical LLMs** (Large Language Models for clinical-scientific texts) are Transformers trained on **PubMed, medical articles, and clinical data**.  \n",
    "They can:\n",
    "- understand and classify medical sentences,  \n",
    "- generate coherent scientific text,  \n",
    "- answer biomedical questions.  \n",
    "\n",
    "We will now look at two examples:\n",
    "1. **BioGPT** for generating a textual response,  \n",
    "2. **PubMedBERT** for classifying medical text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981c8bc",
   "metadata": {},
   "source": [
    "## Example with BioGPT (text generation)\n",
    "\n",
    "BioGPT is a *causal* model (like GPT-2) trained on biomedical articles.  \n",
    "It is capable of **generating coherent and plausible text** based on a medical prompt.\n",
    "\n",
    "#### Objective\n",
    "Generate a brief medical description or explanation from an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c56097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of input_ids: tensor([    2,    18,   151,     5,     6, 10319,   131,    10,   502,   101,\n",
      "           21,    14,   842,  6404,    10])\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and the BioGPT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/BioGPT\")\n",
    "\n",
    "# Input prompt — the model will continue this biomedical text\n",
    "prompt = (\n",
    "    \"The role of the BRCA1 gene in breast cancer is a critical topic in oncology research. \"\n",
    "    \"Several studies have shown that BRCA1 is involved in DNA repair and tumor suppression. \"\n",
    "    \"In particular,\"\n",
    ")\n",
    "\n",
    "# Tokenization → converts text into numerical tensors\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Example of input_ids:\", inputs['input_ids'][0][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d424d-cc08-4094-b4d4-0fed43f9fb81",
   "metadata": {},
   "source": [
    "#### Interpretation \n",
    "\n",
    "The **tokenizer** has transformed the input text into a numerical representation that is readable by the model.\n",
    "\n",
    "- **`input_ids`** → is the sequence of numerical IDs corresponding to the tokens (words or subwords).  \n",
    "  Each number represents a term in BioGPT's internal vocabulary.  \n",
    "\n",
    "In summary:\n",
    "> The tokenizer is the bridge between human language and computational language —  \n",
    "> it converts words into numbers, allowing the model to compute relationships and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89b78daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text from BioGPT:\n",
      "\n",
      "The role of the BRCA1 gene in breast cancer is a critical topic in oncology research. Several studies have shown that BRCA1 is involved in DNA repair and tumor suppression. In particular, BRCA1 is involved in homologous recombination (HR), a process that is essential for the repair of DNA double-strand breaks (DSBs).\n"
     ]
    }
   ],
   "source": [
    "# Biomedical Text Generation\n",
    "output_tokens = model.generate(\n",
    "    **inputs,               # pass the tokenized inputs to the model\n",
    "    max_length=120,         # maximum length of generated sequence (in tokens)\n",
    "    temperature=0.8,        # controls “creativity” → higher = more diverse output\n",
    "    top_p=0.9               # nucleus sampling: considers only the most probable 90% of tokens\n",
    ")\n",
    "\n",
    "# Decoding → converts numeric IDs back to human-readable text\n",
    "generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\"\"\\nGenerated text from BioGPT:\\n\"\"\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925734e",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "- The **input** is a medical prompt (“The role of BRCA1 gene…”).  \n",
    "- The **tokenizer** converts it into numerical IDs for the model.  \n",
    "- The **BioGPT model** generates a plausible textual continuation.  \n",
    "- The **output** is coherent biomedical text, useful for *question answering* or *text summarization* applications.\n",
    "\n",
    "> BioGPT does not truly “understand” the content, but generates text based on learned linguistic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb59d21",
   "metadata": {},
   "source": [
    "## Example with PubMedBERT (Text Classification)\n",
    "\n",
    "PubMedBERT is a BERT-based model, trained from scratch on scientific texts from PubMed.  \n",
    "Here, we use it to **classify** a biomedical text in a straightforward manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eebf3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted label: Disease-related\n",
      "Model confidence: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Create a classification pipeline using BioBERT\n",
    "classifier = pipeline(\"text-classification\", model=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "text = \"Aspirin is used to treat cardiovascular diseases and prevent heart attacks.\"\n",
    "\n",
    "# Performs tokenization, model inference, and decoding automatically\n",
    "result = classifier(text)\n",
    "\n",
    "# Interpretation\n",
    "label = result[0]['label']\n",
    "score = result[0]['score']\n",
    "\n",
    "# Demonstrative label mapping (example)\n",
    "label_map = {\"LABEL_0\": \"Non disease-related\", \"LABEL_1\": \"Disease-related\"}\n",
    "mapped_label = label_map.get(label, label)\n",
    "\n",
    "print(f\"\\nPredicted label: {mapped_label}\")\n",
    "print(f\"Model confidence: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b04a3",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "- The **input** is a biomedical sentence.  \n",
    "- The **pipeline** automatically applies tokenization, inference, and decoding.  \n",
    "- The **output** returns a label and a probability (how confident the model is).  \n",
    "\n",
    "> This flow simplifies the practical use of biomedical LLMs, without the need to write additional code for tensors or tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a6a5e-3879-4179-92c9-925b4e9a4e47",
   "metadata": {},
   "source": [
    "##### Note on the BiomedNLP-PubMedBERT Model\n",
    "\n",
    "The model **`microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext`** has been generally trained on biomedical texts (abstracts and full texts from PubMed).  \n",
    "However, it is **not specifically fine-tuned** for the classification of \"disease-related vs non-disease\".  \n",
    "\n",
    "To achieve accurate results for this type of task, it is necessary to perform **supervised fine-tuning**, that is, to retrain the model on a labeled dataset with the two desired classes.  \n",
    "In the absence of fine-tuning, the model provides only a **rough estimate** based on its general knowledge of biomedical language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f390a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have seen two modes of use:\n",
    "1. **BioGPT** → generation of coherent biomedical text.  \n",
    "2. **PubMedBERT** → classification or understanding of clinical sentences.  \n",
    "\n",
    "### General Flow:\n",
    "```\n",
    "Text → Tokenizer → Model → Readable Output\n",
    "```\n",
    "\n",
    "### Key Points to Remember\n",
    "- `from_pretrained()` automatically loads architecture and weights.  \n",
    "- `AutoTokenizer` converts text into numerical input.  \n",
    "- `pipeline()` combines all steps into a single command.\n",
    "\n",
    "Now you can experiment with other tasks: *NER*, *question answering*, *summarization* — all following the same logic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
